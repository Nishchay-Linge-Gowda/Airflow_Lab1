# ğŸš€ Airflow Lab 1 â€” Employee Productivity Clustering Pipeline

This project demonstrates an end-to-end **MLOps workflow using Apache Airflow**, where a Machine Learning pipeline automatically:

1. **Loads an Employee Productivity dataset**
2. **Preprocesses & scales the numeric features**
3. **Builds a KMeans clustering model**
4. **Generates the SSE curve for the Elbow Method**
5. **Saves the final model**
6. **Loads the model and makes a test prediction**

All tasks are orchestrated using **Apache Airflow DAGs**, containerized via **Docker Compose**, and the ML logic is fully modular inside `lab.py`.

---

## ğŸ“‚ Project Structure

```
LAB_1/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ airflow.py                       # Main Airflow DAG
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ employee_productivity.csv     # Your custom dataset
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ employee_model.sav            # Auto-generated model file
â”‚   â”‚
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ lab.py                        # All ML logic (ETL + ML pipeline)
â”‚
â”œâ”€â”€ logs/                                  # Airflow logs
â”œâ”€â”€ plugins/
â”‚
â”œâ”€â”€ docker-compose.yaml                     # Airflow infra
â”œâ”€â”€ setup.sh
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset Used â€” Employee Productivity

Your dataset contains employee performance metrics:

| Column Name          | Description                         |
|----------------------|-------------------------------------|
| Hours_Worked         | Total hours worked in a week        |
| Tasks_Completed      | Number of finished tasks            |
| Errors_Made          | Mistakes made by the employee       |
| Productivity_Score   | Calculated performance score        |

This dataset is used for **unsupervised clustering**.

---

## ğŸ¯ Workflow Steps (DAG Logic)

### **1ï¸âƒ£ load_data_task**
- Reads `employee_productivity.csv`
- Serializes dataframe using pickle â†’ Base64
- Pushes it to XCom

### **2ï¸âƒ£ data_preprocessing_task**
- Drops missing values
- Selects numeric columns:
  - Hours_Worked  
  - Tasks_Completed  
  - Errors_Made  
  - Productivity_Score
- Scales using MinMaxScaler
- Returns Base64-encoded scaled array

### **3ï¸âƒ£ build_save_model_task**
- Runs KMeans for `k = 1 to 10`
- Stores SSE values for Elbow Method
- Saves final model (`employee_model.sav`)

### **4ï¸âƒ£ load_model_task**
- Loads saved model
- Runs the Elbow Method (KneeLocator)
- Loads test row from dataset
- Outputs a predicted cluster

---

## ğŸ³ Running the Project (Full Steps)

### **1ï¸âƒ£ Start Airflow environment**
```bash
docker compose up airflow-init
```

### **2ï¸âƒ£ Start all Airflow services**
```bash
docker compose up
```

### **3ï¸âƒ£ Open Airflow UI**
```
http://localhost:8080
```

Login:
- Username: `airflow`
- Password: `airflow`

### **4ï¸âƒ£ Trigger DAG**
- Go to **DAGs â†’ Airflow_Lab1 â†’ Trigger DAG**
- Watch each task turn **green**

---

## ğŸ“¦ Model Output

After DAG runs, the model is saved at:

```
dags/model/employee_model.sav
```

A predicted cluster label appears in the logs of `load_model_task`.

---

## ğŸ§  Technologies Used

- **Apache Airflow**
- **Docker & Docker Compose**
- **Python 3.12**
- **Pandas**
- **Scikit-Learn**
- **KMeans Clustering**
- **KneeLocator (Elbow Method)**
- **XCom with Base64 Encoding**

---

## ğŸ“ Example DAG Graph

```
load_data_task  
        â†“  
data_preprocessing_task  
        â†“  
build_save_model_task  
        â†“  
load_model_task
```


